import os
import sys
import pandas as pd
import pickle
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
# Ø°Ø®ÛŒØ±Ù‡ tokenizer Ø¨Ù‡ ØµÙˆØ±Øª JSON Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø§Ù†Ø¯Ø±ÙˆÛŒØ¯
from tensorflow.keras.preprocessing.text import tokenizer_from_json
import json
# ØªÙ†Ø¸ÛŒÙ… Ø®Ø±ÙˆØ¬ÛŒ ØªØ±Ù…ÛŒÙ†Ø§Ù„ Ø¨Ø±Ø§ÛŒ UTF-8
try:
    sys.stdout.reconfigure(encoding='utf-8')
except:
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# Ù…Ø³ÛŒØ±Ù‡Ø§
MODEL_DIR = "train-model"
CSV_PATH = os.path.join(MODEL_DIR, "train.csv")
MODEL_PATH = os.path.join(MODEL_DIR, "intent_model.h5")
TOKENIZER_PATH = os.path.join(MODEL_DIR, "tokenizer.pkl")
LABEL_ENCODER_PATH = os.path.join(MODEL_DIR, "label_encoder.pkl")
tokenizer_json_path = os.path.join(MODEL_DIR, "tokenizer.json")

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù¾ÙˆØ´Ù‡ Ù…Ø¯Ù„
os.makedirs(MODEL_DIR, exist_ok=True)

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„ CSV
if not os.path.exists(CSV_PATH):
    print(f"âŒ ÙØ§ÛŒÙ„ {CSV_PATH} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
    sys.exit(1)

# Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_csv(CSV_PATH)

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
texts = df['text'].astype(str).tolist()
labels = df['command_type'].astype(str).tolist()

# ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
vocab_size = 1000
max_length = 10
oov_token = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

tokenizer_json = tokenizer.to_json()
# Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)
num_classes = len(label_encoder.classes_)

# Ø³Ø§Ø®Øª Ù…Ø¯Ù„
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Ø¢Ù…ÙˆØ²Ø´
print("ğŸ¯ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
model.fit(padded_sequences, encoded_labels, epochs=50, verbose=2)
print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ú©Ø§Ù…Ù„ Ø´Ø¯.")

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
model.save(MODEL_PATH)
print(f"âœ… Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ â†’ {MODEL_PATH}")


with open(tokenizer_json_path, 'w', encoding='utf-8') as f:
    f.write(tokenizer_json)

print(f"âœ… tokenizer.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ â†’ {tokenizer_json_path}")

# Ø°Ø®ÛŒØ±Ù‡ Tokenizer Ùˆ LabelEncoder
with open(TOKENIZER_PATH, 'wb') as f:
    pickle.dump(tokenizer, f)
with open(LABEL_ENCODER_PATH, 'wb') as f:
    pickle.dump(label_encoder, f)
print("âœ… Tokenizer Ùˆ LabelEncoder Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
